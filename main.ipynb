{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ToanLe\\AppData\\Roaming\\Python\\Python37\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from data_normalizer import normalize\n",
    "from dotenv import load_dotenv\n",
    "import constants\n",
    "import prettytable\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts(text):\n",
    "    with open('data/intent_alias_data.json', encoding=\"utf8\") as f:\n",
    "        dictionary = json.load(f)\n",
    "\n",
    "    out = {}\n",
    "    for concept in dictionary:\n",
    "        for alias in sorted(dictionary[concept], key=len, reverse=1):\n",
    "            if alias in text:\n",
    "                out[concept] = alias\n",
    "                break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_MODEL_BERT = \"phobert_large\"\n",
    "NER_MODEL_BILSTM = \"BiLSTM\"\n",
    "NER_MODEL_BILSTM_CRF = \"BiLSTM+CRF\"\n",
    "\n",
    "INTENT_MODEL_ONE_VS_REST = \"onevsrest\"\n",
    "\n",
    "\n",
    "def extract_ner(text, model=NER_MODEL_BERT):\n",
    "    \"\"\"\n",
    "    Input Arguments:\n",
    "        - text : the sentence which will be extracted NER\n",
    "    \"\"\"\n",
    "    ner_service_url = os.getenv(\"NER_SERVICE_URL\",\n",
    "                                default=\"http://localhost:8001/api/v1/ner\")\n",
    "\n",
    "    data = {'model': model, 'text': text}\n",
    "\n",
    "    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "\n",
    "    r = requests.post(ner_service_url, data=json.dumps(data), headers=headers)\n",
    "\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_postprocess(entities_raw):\n",
    "    \"\"\"Postprocess for NER: \n",
    "    1. normalize values\n",
    "    2. only accept the related information with previous question\n",
    "\n",
    "    Args:\n",
    "        entities_raw (Dict): result of NER service\n",
    "\n",
    "    Returns:\n",
    "        Dict, Dict: raw and normalized entities\n",
    "    \"\"\"\n",
    "    entities_normed = dict()\n",
    "    entities_raw_out = dict()\n",
    "    # print(\"entities_raw: \", entities_raw)\n",
    "    for entity in entities_raw:\n",
    "        key = entity['label']\n",
    "        if key == 'O': continue\n",
    "\n",
    "        value_raw = entity['content']\n",
    "        if key not in entities_raw_out.keys():\n",
    "            entities_raw_out[key] = [value_raw]\n",
    "        else:\n",
    "            entities_raw_out[key].append(value_raw)\n",
    "\n",
    "        value_normed = normalize(value_raw, key)\n",
    "        if key not in entities_normed.keys():\n",
    "            entities_normed[key] = [value_normed]\n",
    "        else:\n",
    "            entities_normed[key].append(value_normed)\n",
    "\n",
    "    return entities_raw_out, entities_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    entities_raw = extract_ner(text)\n",
    "    _, entities = ner_postprocess(entities_raw)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_query(concepts_keys, evidences_keys, targets, evidences):\n",
    "    out = \"\"\"\n",
    "    MATCH {} \n",
    "    WHERE {}\n",
    "    RETURN {}\n",
    "    LIMIT 30\n",
    "        \"\"\"\n",
    "    conditions = []\n",
    "    matched_labels = []\n",
    "    idx_target_labels = []\n",
    "    condition_labels = []\n",
    "    returned_labels = []\n",
    "\n",
    "    # Define matched labels.\n",
    "    if not concepts_keys:\n",
    "        matched_labels = ['n']\n",
    "    else:\n",
    "        for idx, target in enumerate(concepts_keys):\n",
    "            idx_target_labels.append('n' + str(idx))\n",
    "            matched_labels.append('(n' + str(idx) + ':' + target + ')')\n",
    "\n",
    "    # Define returned labels.\n",
    "    returned_labels = matched_labels.copy()\n",
    "\n",
    "    ##\n",
    "    # Define condition labels.\n",
    "    # #\n",
    "    attrs = [constants.LABEL_REAL_ESTATE_TYPE, constants.LABEL_REAL_ESTATE_SUB_TYPE, \\\n",
    "            constants.LABEL_POSITION, constants.LABEL_DIRECTION, \\\n",
    "            constants.LABEL_FRONT_LENGTH, constants.LABEL_ROAD_WIDTH, \\\n",
    "            constants.LABEL_FLOOR, constants.LABEL_BED_ROOM, constants.LABEL_LIVING_ROOM, constants.LABEL_BATH_ROOM, \\\n",
    "            constants.LABEL_SURROUNDING, constants.LABEL_PROJECT_NAME, \\\n",
    "            constants.LABEL_LEGAL, constants.LABEL_TRANSACTION]\n",
    "\n",
    "    key2dbcol = {\n",
    "        'tang': constants.LABEL_FLOOR,\n",
    "        'ban cong': constants.LABEL_FLOOR_BAN_CONG,\n",
    "        'gac': constants.LABEL_FLOOR_GAC,\n",
    "        'ham': constants.LABEL_FLOOR_HAM,\n",
    "        'lung': constants.LABEL_FLOOR_LUNG,\n",
    "        'san thuong': constants.LABEL_FLOOR_SAN_THUONG,\n",
    "        'tret': constants.LABEL_FLOOR_TRET\n",
    "    }\n",
    "\n",
    "    for attr in attrs:\n",
    "        if attr in evidences:\n",
    "            if attr == constants.LABEL_FLOOR:\n",
    "                for val in evidences[attr]:\n",
    "                    target_k = key2dbcol[val['type']]\n",
    "                    target_v = val['value']\n",
    "\n",
    "                    for match_label in matched_labels:\n",
    "                        if target_k in match_label:\n",
    "                            idx_condition_node = match_label.split(\n",
    "                                '(')[1].split(':')[0]\n",
    "                            returned_labels.remove(match_label)\n",
    "\n",
    "                    # conditions.append(f\"{target_k} = '{target_v}'\")\n",
    "                    conditions.append(\n",
    "                        f\"{idx_condition_node}.individual = '{target_v}'\")\n",
    "            else:\n",
    "                # if attr == constants.LABEL_REAL_ESTATE_TYPE or attr == constants.LABEL_REAL_ESTATE_SUB_TYPE:\n",
    "                #     attr = \"House\"\n",
    "\n",
    "                for match_label in matched_labels:\n",
    "                    if attr in match_label:\n",
    "                        idx_condition_node = match_label.split('(')[1].split(\n",
    "                            ':')[0]\n",
    "                        returned_labels.remove(match_label)\n",
    "\n",
    "                # conditions.append(f\"{attr}.individual = '{evidences[attr][0]}'\")\n",
    "                conditions.append(\n",
    "                    f\"{idx_condition_node}.individual = '{evidences[attr][0]}'\"\n",
    "                )\n",
    "\n",
    "    ##\n",
    "    # Condition for city, district, ward, street.\n",
    "    # #\n",
    "    loc_attrs = [\n",
    "        constants.LABEL_DISTRICT, constants.LABEL_CITY, constants.LABEL_WARD,\n",
    "        constants.LABEL_STREET\n",
    "    ]\n",
    "\n",
    "    for attr in loc_attrs:\n",
    "        if attr in evidences:\n",
    "            for match_label in matched_labels:\n",
    "                if attr in match_label:\n",
    "                    idx_condition_node = match_label.split('(')[1].split(\n",
    "                        ':')[0]\n",
    "\n",
    "                    returned_labels.remove(match_label)\n",
    "\n",
    "            conditions.append(\n",
    "                f\"{idx_condition_node}.individual = '{evidences[attr][0]}'\")\n",
    "\n",
    "    ##\n",
    "    # Condition for price.\n",
    "    # #\n",
    "    PRICE_OFFSET_CONST = 0.1\n",
    "    # print(\"evidences-1: \", evidences)\n",
    "    # print()\n",
    "    # price_title = constants.LABEL_PRICE\n",
    "\n",
    "    if constants.LABEL_PRICE in evidences:\n",
    "        # print(\"evidences-2: \", evidences)\n",
    "\n",
    "        for ele in evidences[constants.LABEL_PRICE][:1]:\n",
    "            low, high = ele\n",
    "\n",
    "            if high is None:\n",
    "                high = low + low * PRICE_OFFSET_CONST\n",
    "                low = low - low * PRICE_OFFSET_CONST\n",
    "\n",
    "            for match_label in matched_labels:\n",
    "                if constants.LABEL_PRICE in match_label:\n",
    "                    idx_condition_node = match_label.split('(')[1].split(\n",
    "                        ':')[0]\n",
    "\n",
    "                    returned_labels.remove(match_label)\n",
    "\n",
    "            # conditions.append(f\"{constants.LABEL_PRICE} BETWEEN {low} AND {high}\")\n",
    "            conditions.append(\n",
    "                f\"{idx_condition_node}.individual = BETWEEN ' {low} ' AND ' {high} '\"\n",
    "            )\n",
    "\n",
    "    ##\n",
    "    # Condition for area.\n",
    "    # #\n",
    "    AREA_OFFSET_CONST = 0.1\n",
    "\n",
    "    if constants.LABEL_AREA in evidences:\n",
    "        for ele in evidences[constants.LABEL_AREA][:1]:\n",
    "            low, high = ele\n",
    "\n",
    "            if high is None:\n",
    "                high = low + low * AREA_OFFSET_CONST\n",
    "                low = low - low * AREA_OFFSET_CONST\n",
    "\n",
    "            for match_label in matched_labels:\n",
    "                if constants.LABEL_AREA in match_label:\n",
    "                    idx_condition_node = match_label.split('(')[1].split(\n",
    "                        ':')[0]\n",
    "\n",
    "                    returned_labels.remove(match_label)\n",
    "\n",
    "            # conditions.append(f\"{constants.LABEL_AREA} BETWEEN {low} AND {high}\")\n",
    "            conditions.append(\n",
    "                f\"{idx_condition_node}.individual = BETWEEN ' {low} ' AND ' {high} '\"\n",
    "            )\n",
    "\n",
    "    ##\n",
    "    # Condition for usage.\n",
    "    # #\n",
    "    if constants.LABEL_USAGE in evidences:\n",
    "        for match_label in matched_labels:\n",
    "            if constants.LABEL_USAGE in match_label:\n",
    "                idx_condition_node = match_label.split('(')[1].split(':')[0]\n",
    "\n",
    "                returned_labels.remove(match_label)\n",
    "\n",
    "        # conditions.append(\"({})\".format(\" OR \".join([f\"{constants.LABEL_USAGE} LIKE '%, {x},%' + 'OR {constants.LABEL_USAGE} LIKE '{x}, %' OR {constants.LABEL_USAGE} LIKE '%, {x}'\" for x in evidences[constants.LABEL_USAGE]])))\n",
    "        conditions.append(\"({})\".format(\" OR \".join([\n",
    "            f\"{idx_condition_node}.individual LIKE '%, {x},%' + 'OR {idx_condition_node}.individual LIKE '{x}, %' OR {idx_condition_node}.individual LIKE '%, {x}'\"\n",
    "            for x in evidences[constants.LABEL_USAGE]\n",
    "        ])))\n",
    "\n",
    "    ##\n",
    "    # Adjust returned labels\n",
    "    # #\n",
    "    adjusted_return_labels = []\n",
    "\n",
    "    for return_label in returned_labels:\n",
    "        return_label = f\"{return_label.split('(')[1].split(':')[0]}.individual as {return_label.split(':')[1].split(')')[0]}\"\n",
    "\n",
    "        adjusted_return_labels.append(return_label)\n",
    "\n",
    "    return out.format(\n",
    "        ', '.join(matched_labels),\n",
    "        ' AND '.join([f\"{x}\" for x in conditions]),\n",
    "        ', '.join(adjusted_return_labels),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_query_ontology(text):\n",
    "    table = prettytable.PrettyTable([\"Step\", \"Result\"])\n",
    "    table.add_row([\"Input\", text])\n",
    "\n",
    "    concepts = get_concepts(text)\n",
    "    # print(\"concepts: \", concepts)\n",
    "\n",
    "    evidences = get_entities(text)\n",
    "\n",
    "    table.add_rows([[\"Match alias\", concepts], [\"Find individuals\",\n",
    "                                                evidences]])\n",
    "\n",
    "    targets = list(set(concepts.keys()).difference(set(evidences.keys())))\n",
    "    table.add_row([\"Target concepts\", targets])\n",
    "\n",
    "    query = gen_query(concepts.keys(), evidences.keys(), targets, evidences)\n",
    "    table.add_row([\"Query\", query])\n",
    "\n",
    "    print(table)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities_raw:  [{'content': 'nhà', 'label': 'real_estate_type'}, {'content': 'hoặc', 'label': 'O'}, {'content': 'căn hộ', 'label': 'real_estate_type'}, {'content': 'giá khoảng', 'label': 'O'}, {'content': '2 tỷ', 'label': 'price'}, {'content': 'thì', 'label': 'O'}, {'content': 'mua', 'label': 'transaction'}, {'content': 'ở quận nào', 'label': 'O'}]\n",
      "evidences-2:  {'real_estate_type': ['nha', 'can ho'], 'price': [(2000000000.0, None)], 'transaction': ['mua']}\n",
      "+------------------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|       Step       |                                                            Result                                                           |\n",
      "+------------------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|      Input       |                                      nhà hoặc căn hộ giá khoảng 2 tỷ thì mua ở quận nào                                     |\n",
      "|   Match alias    |                    {'real_estate_type': 'nhà', 'transaction': 'mua', 'price': 'giá', 'district': 'quận'}                    |\n",
      "| Find individuals |               {'real_estate_type': ['nha', 'can ho'], 'price': [(2000000000.0, None)], 'transaction': ['mua']}              |\n",
      "| Target concepts  |                                                         ['district']                                                        |\n",
      "|      Query       |                                                                                                                             |\n",
      "|                  |                            MATCH (n0:real_estate_type), (n1:transaction), (n2:price), (n3:district)                         |\n",
      "|                  |     WHERE n0.individual = 'nha' AND n1.individual = 'mua' AND n2.individual = BETWEEN ' 1800000000.0 ' AND ' 2200000000.0 ' |\n",
      "|                  |                                                 RETURN n3.individual as district                                            |\n",
      "|                  |                                                             LIMIT 30                                                        |\n",
      "|                  |                                                                                                                             |\n",
      "+------------------+-----------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# text = \"nhà ở quận 1 thường có giá khoảng bao nhiêu\"\n",
    "text = \"nhà hoặc căn hộ giá khoảng 2 tỷ thì mua ở quận nào\"\n",
    "\n",
    "cqlNodeQuery = gen_query_ontology(text)\n",
    "# i = 0\n",
    "# print(cqlNodeQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect neo4j DB.\n",
    "driver = GraphDatabase.driver('bolt://localhost:7687',\n",
    "                              auth=('neo4j', 'password'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    with driver.session(database=\"htdb\") as session:\n",
    "        results = session.run(query)\n",
    "\n",
    "        table_results = prettytable.PrettyTable(results.keys())\n",
    "        for r in results:\n",
    "            table_results.add_row(r.values())\n",
    "\n",
    "        return table_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|   House   |     Price      |\n",
      "+-----------+----------------+\n",
      "| shophouse |  1000000000.0  |\n",
      "| shophouse | 22000000000.0  |\n",
      "| shophouse |  5000000000.0  |\n",
      "| shophouse |  4000000000.0  |\n",
      "| shophouse | 130000000000.0 |\n",
      "| shophouse |  3000000000.0  |\n",
      "| shophouse |  6000000000.0  |\n",
      "| shophouse |  560000000.0   |\n",
      "| shophouse | 17000000000.0  |\n",
      "| shophouse |  2000000000.0  |\n",
      "| shophouse |  8000000000.0  |\n",
      "| shophouse |  800000000.0   |\n",
      "| shophouse |  9000000000.0  |\n",
      "| shophouse | 14000000000.0  |\n",
      "| shophouse |  320000000.0   |\n",
      "| shophouse | 35000000000.0  |\n",
      "| shophouse | 10000000000.0  |\n",
      "| shophouse |  7000000000.0  |\n",
      "| shophouse |  780000000.0   |\n",
      "| shophouse | 15000000000.0  |\n",
      "| shophouse |   30000000.0   |\n",
      "| shophouse |  630000000.0   |\n",
      "| shophouse | 19000000000.0  |\n",
      "| shophouse |  550000000.0   |\n",
      "| shophouse |   2000000.0    |\n",
      "| shophouse |  670000000.0   |\n",
      "| shophouse | 13000000000.0  |\n",
      "| shophouse |  540000000.0   |\n",
      "| shophouse |  890000000.0   |\n",
      "| shophouse |  760000000.0   |\n",
      "+-----------+----------------+\n"
     ]
    }
   ],
   "source": [
    "result_data = run_query(cqlNodeQuery)\n",
    "\n",
    "print(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
