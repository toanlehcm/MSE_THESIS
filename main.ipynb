{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from data_normalizer import normalize\n",
    "from dotenv import load_dotenv\n",
    "import constants\n",
    "import prettytable\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts(text):\n",
    "    with open('data/intent_alias_data.json', encoding=\"utf8\") as f:\n",
    "        dictionary = json.load(f)\n",
    "\n",
    "    out = {}\n",
    "    for concept in dictionary:\n",
    "        for alias in sorted(dictionary[concept], key=len, reverse=1):\n",
    "            if alias in text:\n",
    "                out[concept] = alias\n",
    "                break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_MODEL_BERT = \"phobert_large\"\n",
    "NER_MODEL_BILSTM = \"BiLSTM\"\n",
    "NER_MODEL_BILSTM_CRF = \"BiLSTM+CRF\"\n",
    "\n",
    "INTENT_MODEL_ONE_VS_REST = \"onevsrest\"\n",
    "\n",
    "\n",
    "def extract_ner(text, model=NER_MODEL_BERT):\n",
    "    \"\"\"\n",
    "    Input Arguments:\n",
    "        - text : the sentence which will be extracted NER\n",
    "    \"\"\"\n",
    "    ner_service_url = os.getenv(\"NER_SERVICE_URL\",\n",
    "                                default=\"http://localhost:8001/api/v1/ner\")\n",
    "\n",
    "    data = {'model': model, 'text': text}\n",
    "\n",
    "    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "\n",
    "    r = requests.post(ner_service_url, data=json.dumps(data), headers=headers)\n",
    "\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_postprocess(entities_raw):\n",
    "    \"\"\"Postprocess for NER: \n",
    "    1. normalize values\n",
    "    2. only accept the related information with previous question\n",
    "\n",
    "    Args:\n",
    "        entities_raw (Dict): result of NER service\n",
    "\n",
    "    Returns:\n",
    "        Dict, Dict: raw and normalized entities\n",
    "    \"\"\"\n",
    "    entities_normed = dict()\n",
    "    entities_raw_out = dict()\n",
    "\n",
    "    for entity in entities_raw:\n",
    "        key = entity['label']\n",
    "        if key == 'O': continue\n",
    "\n",
    "        value_raw = entity['content']\n",
    "        if key not in entities_raw_out.keys():\n",
    "            entities_raw_out[key] = [value_raw]\n",
    "        else:\n",
    "            entities_raw_out[key].append(value_raw)\n",
    "\n",
    "        value_normed = normalize(value_raw, key)\n",
    "        if key not in entities_normed.keys():\n",
    "            entities_normed[key] = [value_normed]\n",
    "        else:\n",
    "            entities_normed[key].append(value_normed)\n",
    "\n",
    "    return entities_raw_out, entities_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    entities_raw = extract_ner(text)\n",
    "    _, entities = ner_postprocess(entities_raw)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_query(concepts_keys, evidences_keys, targets, evidences):\n",
    "    out = \"\"\"\n",
    "    MATCH {} \n",
    "    WHERE {}\n",
    "    RETURN {}\n",
    "    LIMIT 30\n",
    "        \"\"\"\n",
    "    conditions = []\n",
    "    matched_labels = []\n",
    "    matched_labels_query = []\n",
    "    idx_target_labels = []\n",
    "    condition_labels = []\n",
    "    returned_labels = []\n",
    "\n",
    "    # Define matched labels.\n",
    "    if not concepts_keys:\n",
    "        matched_labels = ['n']\n",
    "    else:\n",
    "        for idx, target in enumerate(concepts_keys):\n",
    "            idx_target_labels.append('n' + str(idx))\n",
    "            matched_labels.append('(n' + str(idx) + ':' + target + ')')\n",
    "            matched_labels_query.append('(n' + str(idx) + ':' +\n",
    "                                        target.title() + ')')\n",
    "\n",
    "    # Define returned labels.\n",
    "    returned_labels = matched_labels.copy()\n",
    "\n",
    "    ##\n",
    "    # Define condition labels.\n",
    "    # #\n",
    "    attrs = [constants.LABEL_REAL_ESTATE_TYPE, constants.LABEL_REAL_ESTATE_SUB_TYPE, \\\n",
    "            constants.LABEL_POSITION, constants.LABEL_DIRECTION, \\\n",
    "            constants.LABEL_FRONT_LENGTH, constants.LABEL_ROAD_WIDTH, \\\n",
    "            constants.LABEL_FLOOR, constants.LABEL_BED_ROOM, constants.LABEL_LIVING_ROOM, constants.LABEL_BATH_ROOM, \\\n",
    "            constants.LABEL_SURROUNDING, constants.LABEL_PROJECT_NAME, \\\n",
    "            constants.LABEL_LEGAL, constants.LABEL_TRANSACTION]\n",
    "\n",
    "    key2dbcol = {\n",
    "        'tang': constants.LABEL_FLOOR,\n",
    "        'ban cong': constants.LABEL_FLOOR_BAN_CONG,\n",
    "        'gac': constants.LABEL_FLOOR_GAC,\n",
    "        'ham': constants.LABEL_FLOOR_HAM,\n",
    "        'lung': constants.LABEL_FLOOR_LUNG,\n",
    "        'san thuong': constants.LABEL_FLOOR_SAN_THUONG,\n",
    "        'tret': constants.LABEL_FLOOR_TRET\n",
    "    }\n",
    "\n",
    "    for attr in attrs:\n",
    "        if attr in evidences:\n",
    "            if attr == constants.LABEL_FLOOR:\n",
    "                for val in evidences[attr]:\n",
    "                    target_k = key2dbcol[val['type']]\n",
    "                    target_v = val['value']\n",
    "\n",
    "                    for match_label in matched_labels:\n",
    "                        if target_k in match_label:\n",
    "                            idx_condition_node = match_label.split(\n",
    "                                '(')[1].split(':')[0]\n",
    "                            returned_labels.remove(match_label)\n",
    "\n",
    "                    conditions.append(\n",
    "                        f\"{idx_condition_node}.individual = '{target_v}'\")\n",
    "            else:\n",
    "                for match_label in matched_labels:\n",
    "                    if attr in match_label:\n",
    "                        idx_condition_node = match_label.split('(')[1].split(\n",
    "                            ':')[0]\n",
    "                        returned_labels.remove(match_label)\n",
    "\n",
    "                conditions.append(\n",
    "                    f\"{idx_condition_node}.individual = '{evidences[attr][0]}'\"\n",
    "                )\n",
    "\n",
    "    ##\n",
    "    # Condition for city, district, ward, street.\n",
    "    # #\n",
    "    loc_attrs = [\n",
    "        constants.LABEL_DISTRICT, constants.LABEL_CITY, constants.LABEL_WARD,\n",
    "        constants.LABEL_STREET\n",
    "    ]\n",
    "\n",
    "    for attr in loc_attrs:\n",
    "        if attr in evidences:\n",
    "            for match_label in matched_labels:\n",
    "                if attr in match_label:\n",
    "                    idx_condition_node = match_label.split('(')[1].split(\n",
    "                        ':')[0]\n",
    "\n",
    "                    returned_labels.remove(match_label)\n",
    "\n",
    "            conditions.append(\n",
    "                f\"{idx_condition_node}.individual = '{evidences[attr][0]}'\")\n",
    "\n",
    "    ##\n",
    "    # Condition for price.\n",
    "    # #\n",
    "    PRICE_OFFSET_CONST = 0.1\n",
    "\n",
    "    if constants.LABEL_PRICE in evidences:\n",
    "        for ele in evidences[constants.LABEL_PRICE][:1]:\n",
    "            low, high = ele\n",
    "\n",
    "            if high is None:\n",
    "                high = low + low * PRICE_OFFSET_CONST\n",
    "                low = low - low * PRICE_OFFSET_CONST\n",
    "\n",
    "            for match_label in matched_labels:\n",
    "                if constants.LABEL_PRICE in match_label:\n",
    "                    idx_condition_node = match_label.split('(')[1].split(\n",
    "                        ':')[0]\n",
    "\n",
    "                    returned_labels.remove(match_label)\n",
    "\n",
    "            conditions.append(\n",
    "                f\"'{low}' <= {idx_condition_node}.individual <= '{high}'\"\n",
    "            )\n",
    "\n",
    "    ##\n",
    "    # Condition for area.\n",
    "    # #\n",
    "    AREA_OFFSET_CONST = 0.1\n",
    "\n",
    "    if constants.LABEL_AREA in evidences:\n",
    "        for ele in evidences[constants.LABEL_AREA][:1]:\n",
    "            low, high = ele\n",
    "\n",
    "            if high is None:\n",
    "                high = low + low * AREA_OFFSET_CONST\n",
    "                low = low - low * AREA_OFFSET_CONST\n",
    "\n",
    "            for match_label in matched_labels:\n",
    "                if constants.LABEL_AREA in match_label:\n",
    "                    idx_condition_node = match_label.split('(')[1].split(\n",
    "                        ':')[0]\n",
    "\n",
    "                    returned_labels.remove(match_label)\n",
    "\n",
    "            conditions.append(\n",
    "                f\"'{low}' <= {idx_condition_node}.individual <= '{high}'\"\n",
    "            )\n",
    "\n",
    "    ##\n",
    "    # Condition for usage.\n",
    "    # #\n",
    "    if constants.LABEL_USAGE in evidences:\n",
    "        for match_label in matched_labels:\n",
    "            if constants.LABEL_USAGE in match_label:\n",
    "                idx_condition_node = match_label.split('(')[1].split(':')[0]\n",
    "\n",
    "                returned_labels.remove(match_label)\n",
    "\n",
    "        conditions.append(\"({})\".format(\" OR \".join([\n",
    "            f\"{idx_condition_node}.individual LIKE '%, {x},%' + 'OR {idx_condition_node}.individual LIKE '{x}, %' OR {idx_condition_node}.individual LIKE '%, {x}'\"\n",
    "            for x in evidences[constants.LABEL_USAGE]\n",
    "        ])))\n",
    "\n",
    "    ##\n",
    "    # Change real_estate_type or real_estate_sub_type to House label.\n",
    "    # #\n",
    "    for idx, label_query in enumerate(matched_labels_query):\n",
    "        if (constants.LABEL_REAL_ESTATE_TYPE).title() in label_query:\n",
    "            matched_labels_query[idx] = label_query.replace(\n",
    "                (constants.LABEL_REAL_ESTATE_TYPE).title(),\n",
    "                (constants.LABEL_HOUSE).title())\n",
    "        elif (constants.LABEL_REAL_ESTATE_SUB_TYPE).title() in label_query:\n",
    "            matched_labels_query[idx] = label_query.replace(\n",
    "                (constants.LABEL_REAL_ESTATE_SUB_TYPE).title(),\n",
    "                (constants.LABEL_HOUSE).title())\n",
    "\n",
    "    ##\n",
    "    # Adjust returned labels\n",
    "    # #\n",
    "    adjusted_return_labels = []\n",
    "\n",
    "    for return_label in returned_labels:\n",
    "        return_label = f\"{return_label.split('(')[1].split(':')[0]}.individual as {(return_label.split(':')[1].split(')')[0]).title()}\"\n",
    "\n",
    "        adjusted_return_labels.append(return_label)\n",
    "\n",
    "    return out.format(\n",
    "        ', '.join(matched_labels_query),\n",
    "        ' AND '.join([f\"{x}\" for x in conditions]),\n",
    "        ', '.join(adjusted_return_labels),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_query_ontology(text):\n",
    "    table = prettytable.PrettyTable([\"Step\", \"Result\"])\n",
    "    table.add_row([\"Input\", text])\n",
    "\n",
    "    concepts = get_concepts(text)\n",
    "\n",
    "    evidences = get_entities(text)\n",
    "\n",
    "    table.add_rows([[\"Match alias\", concepts], [\"Find individuals\",\n",
    "                                                evidences]])\n",
    "\n",
    "    targets = list(set(concepts.keys()).difference(set(evidences.keys())))\n",
    "    table.add_row([\"Target concepts\", targets])\n",
    "\n",
    "    query = gen_query(concepts.keys(), evidences.keys(), targets, evidences)\n",
    "    table.add_row([\"Query\", query])\n",
    "\n",
    "    print(table)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|       Step       |                                                      Result                                                     |\n",
      "+------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|      Input       |                                nhà hoặc căn hộ giá khoảng 2 tỷ thì mua ở quận nào                               |\n",
      "|   Match alias    |             {'real_estate_type': 'căn hộ', 'transaction': 'mua', 'price': 'giá', 'district': 'quận'}            |\n",
      "| Find individuals |         {'real_estate_type': ['nha', 'can ho'], 'price': [(2000000000.0, None)], 'transaction': ['mua']}        |\n",
      "| Target concepts  |                                                   ['district']                                                  |\n",
      "|      Query       |                                                                                                                 |\n",
      "|                  |                            MATCH (n0:House), (n1:Transaction), (n2:Price), (n3:District)                        |\n",
      "|                  |     WHERE n0.individual = 'nha' AND n1.individual = 'mua' AND '1800000000.0' <= n2.individual <= '2200000000.0' |\n",
      "|                  |                                           RETURN n3.individual as District                                      |\n",
      "|                  |                                                       LIMIT 30                                                  |\n",
      "|                  |                                                                                                                 |\n",
      "+------------------+-----------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# text = \"nhà ở quận 1 thường có giá khoảng bao nhiêu\"\n",
    "text = \"nhà hoặc căn hộ giá khoảng 2 tỷ thì mua ở quận nào\"\n",
    "\n",
    "cqlNodeQuery = gen_query_ontology(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect neo4j DB.\n",
    "driver = GraphDatabase.driver('bolt://localhost:7687',\n",
    "                              auth=('neo4j', 'password'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    with driver.session(database=\"htdb\") as session:\n",
    "        results = session.run(query)\n",
    "\n",
    "        table_results = prettytable.PrettyTable(results.keys())\n",
    "        for r in results:\n",
    "            table_results.add_row(r.values())\n",
    "\n",
    "        return table_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     District     |\n",
      "+------------------+\n",
      "|     ha dong      |\n",
      "|        3         |\n",
      "|        10        |\n",
      "|    phu nhuan     |\n",
      "|    long thanh    |\n",
      "|     binh tan     |\n",
      "|     dien ban     |\n",
      "|        1         |\n",
      "|        9         |\n",
      "|    thanh xuan    |\n",
      "|     thu duc      |\n",
      "|     son tra      |\n",
      "|     kien an      |\n",
      "|    chuong my     |\n",
      "|        6         |\n",
      "|   long bien 1    |\n",
      "|     ham tan      |\n",
      "|    can giuoc     |\n",
      "|        8         |\n",
      "|      2 chau      |\n",
      "|  ham thuan bac   |\n",
      "|     hoa vang     |\n",
      "|      go vap      |\n",
      "| thanh pho da lat |\n",
      "|    ninh kieu     |\n",
      "|    lien chieu    |\n",
      "|    thanh tri     |\n",
      "| thanh pho di an  |\n",
      "|     tan phu      |\n",
      "|    hoang mai     |\n",
      "+------------------+\n"
     ]
    }
   ],
   "source": [
    "result_data = run_query(cqlNodeQuery)\n",
    "\n",
    "print(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
