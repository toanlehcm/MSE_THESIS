{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import urllib\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy\n",
    "import spacy\n",
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'neuralcoref' has no attribute 'add_to_pipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1j/j1wcxnpd4_b39gqrtn4jkh300000gn/T/ipykernel_19778/4125780426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneuralcoref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'neuralcoref' has no attribute 'add_to_pipe'"
     ]
    }
   ],
   "source": [
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import opennre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coref_resolution(text):\n",
    "    print(\"coref_resolution\")\n",
    "\n",
    "    \"\"\"Function that executes coreference resolution on a given text\"\"\"\n",
    "    doc = nlp(text)\n",
    "    # fetches tokens with whitespaces from spacy document\n",
    "    tok_list = list(token.text_with_ws for token in doc)\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        # get tokens from representative cluster name\n",
    "        cluster_main_words = set(cluster.main.text.split(' '))\n",
    "        for coref in cluster:\n",
    "            if coref != cluster.main:  # if coreference element is not the representative element of that cluster\n",
    "                if coref.text != cluster.main.text and bool(set(coref.text.split(' ')).intersection(cluster_main_words)) == False:\n",
    "                    # if coreference element text and representative element text are not equal and none of the coreference element words are in representative element. This was done to handle nested coreference scenarios\n",
    "                    tok_list[coref.start] = cluster.main.text + \\\n",
    "                        doc[coref.end-1].whitespace_\n",
    "                    for i in range(coref.start+1, coref.end):\n",
    "                        tok_list[i] = \"\"\n",
    "\n",
    "    return \"\".join(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikifier(text, lang=\"en\", threshold=0.8):\n",
    "    print(\"wikifier\")\n",
    "    \n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", lang),\n",
    "        (\"userKey\", \"tgbdmkpmkluegqfbawcwjywieevmza\"),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"nTopDfValuesToIgnore\", \"100\"), (\"nWordsToIgnoreFromList\", \"100\"),\n",
    "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
    "        (\"support\", \"true\"), (\"ranges\", \"false\"), (\"minLinkFrequency\", \"2\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # Call the Wikifier and read the response.\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # Output the annotations.\n",
    "    results = list()\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        # Filter out desired entity classes\n",
    "        if ('wikiDataClasses' in annotation) and (any([el['enLabel'] in ENTITY_TYPES for el in annotation['wikiDataClasses']])):\n",
    "\n",
    "            # Specify entity label\n",
    "            if any([el['enLabel'] in [\"human\", \"person\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Person'\n",
    "            elif any([el['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Organization'\n",
    "            elif any([el['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Location'\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            results.append({'title': annotation['title'], 'wikiId': annotation['wikiDataItemId'], 'label': label,\n",
    "                            'characters': [(el['chFrom'], el['chTo']) for el in annotation['support']]})\n",
    "    # print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(s):\n",
    "    print(\"strip_punctuation\")\n",
    "    \n",
    "    \"\"\"Removes all punctuation from a string\"\"\"\n",
    "    return ''.join(c for c in s if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_dict(d):\n",
    "    print(\"deduplicate_dict\")\n",
    "    return [dict(y) for y in set(tuple(x.items()) for x in d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ app.route('/')\n",
    "def hello_ie():\n",
    "    print(\"hello_ie\")\n",
    "    \n",
    "    try:\n",
    "        text = request.args.get('text', None)\n",
    "        relation_threshold = request.args.get('relation_threshold', 0.9)\n",
    "        entities_threshold = request.args.get('entities_threshold', 0.8)\n",
    "        coref = request.args.get('coref', True)\n",
    "        if not text:\n",
    "            return 'Missing text parameter'\n",
    "\n",
    "        try:\n",
    "            relation_threshold = float(relation_threshold)\n",
    "            entities_threshold = float(entities_threshold)\n",
    "        except ValueError:\n",
    "            return 'Invalid value for relation or entity threshold parameter'\n",
    "\n",
    "        if coref:\n",
    "            text = coref_resolution(text)\n",
    "\n",
    "        print(text)\n",
    "\n",
    "        relations_list = list()\n",
    "        entities_list = list()\n",
    "\n",
    "        for sentence in nltk.sent_tokenize(text):\n",
    "            sentence = strip_punctuation(sentence)\n",
    "            entities = wikifier(sentence, threshold=entities_threshold)\n",
    "            entities_list.extend(\n",
    "                [{'title': el['title'], 'wikiId': el['wikiId'], 'label': el['label']} for el in entities])\n",
    "            # Iterate over every permutation pair of entities\n",
    "            for permutation in itertools.permutations(entities, 2):\n",
    "                for source in permutation[0]['characters']:\n",
    "                    for target in permutation[1]['characters']:\n",
    "                        # Relationship extraction with OpenNRE\n",
    "                        data = relation_model.infer(\n",
    "                            {'text': sentence, 'h': {'pos': [source[0], source[1] + 1]}, 't': {'pos': [target[0], target[1] + 1]}})\n",
    "                        if data[1] > relation_threshold:\n",
    "                            relations_list.append(\n",
    "                                {'source': permutation[0]['title'], 'target': permutation[1]['title'], 'type': data[0]})\n",
    "\n",
    "        return {'entities': deduplicate_dict(entities_list), 'relations': deduplicate_dict(relations_list)}\n",
    "    except Exception as e:\n",
    "        return 'An error has occured:' + str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
